{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis\n",
        "a. Apply PCA on CC dataset.\n",
        "b. Apply k-means algorithm on the PCA result and report your observation if the silhouette score has\n",
        "improved or not?\n",
        "c. Perform Scaling+PCA+K-Means and report performance.\n"
      ],
      "metadata": {
        "id": "wxZc7jY1EmeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1LWjOTd_LcG",
        "outputId": "822aded8-6171-4356-8f82-fc240fa09bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (PCA): 0.4774953485130043\n",
            "Silhouette Score (Scaling + PCA): 0.2542103280918146\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_path):\n",
        "    # Load the dataset\n",
        "    cc_data = pd.read_csv('CC GENERAL.csv')\n",
        "    return cc_data\n",
        "\n",
        "def preprocess_data(cc_data):\n",
        "    # Drop the 'CUST_ID' column\n",
        "    cc_data = cc_data.drop(columns=['CUST_ID'])\n",
        "\n",
        "    # Handle missing values by filling them with the mean of the column\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    cc_data_imputed = imputer.fit_transform(cc_data)\n",
        "\n",
        "    return cc_data_imputed\n",
        "\n",
        "def apply_pca(data, variance_threshold=0.95):\n",
        "    pca = PCA()\n",
        "    data_pca = pca.fit_transform(data)\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    num_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n",
        "    pca = PCA(n_components=num_components)\n",
        "    data_pca_reduced = pca.fit_transform(data)\n",
        "\n",
        "    return data_pca_reduced, num_components\n",
        "\n",
        "def kmeans_clustering(data, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = kmeans.fit_predict(data)\n",
        "    silhouette_avg = silhouette_score(data, labels)\n",
        "\n",
        "    return silhouette_avg\n",
        "\n",
        "def main(file_path):\n",
        "    cc_data = load_data(file_path)\n",
        "    cc_data_imputed = preprocess_data(cc_data)\n",
        "\n",
        "    # Apply PCA\n",
        "    cc_data_pca_reduced, num_components = apply_pca(cc_data_imputed)\n",
        "    silhouette_avg_pca = kmeans_clustering(cc_data_pca_reduced)\n",
        "\n",
        "    # Perform Scaling + PCA + K-means\n",
        "    scaler = StandardScaler()\n",
        "    cc_data_scaled = scaler.fit_transform(cc_data_imputed)\n",
        "    cc_data_scaled_pca, _ = apply_pca(cc_data_scaled, variance_threshold=0.95)\n",
        "    silhouette_avg_scaled_pca = kmeans_clustering(cc_data_scaled_pca)\n",
        "\n",
        "    print(f\"Silhouette Score (PCA): {silhouette_avg_pca}\")\n",
        "    print(f\"Silhouette Score (Scaling + PCA): {silhouette_avg_scaled_pca}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/mnt/data/CC GENERAL.csv'  # Adjust the file path as needed\n",
        "    main(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pd_speech_features.csv\n",
        "a. Perform Scaling\n",
        "b. Apply PCA (k=3)\n",
        "c. Use SVM to report performance"
      ],
      "metadata": {
        "id": "9uhe1GnHEtt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'pd_speech_features.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Train and evaluate an SVM model using cross-validation\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "scores = cross_val_score(svm, X_pca, y, cv=5)\n",
        "\n",
        "# Report the performance\n",
        "mean_score = scores.mean()\n",
        "std_score = scores.std()\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knX4kvLrCt0-",
        "outputId": "1eadb2d0-e1ce-463c-da63-adce1836f5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.7751\n",
            "Standard Deviation: 0.0199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Linear Discriminant Analysis (LDA) on Iris.csv dataset to reduce dimensionality of data tok=2."
      ],
      "metadata": {
        "id": "-C5J8bN4Eyyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Load the dataset\n",
        "file_path_iris = 'Iris.csv'\n",
        "iris_data = pd.read_csv(file_path_iris)\n",
        "\n",
        "# Separate features and labels\n",
        "X = iris_data.iloc[:, 1:-1]  # Features (excluding the first column and the last column)\n",
        "y = iris_data.iloc[:, -1]    # Labels (last column)\n",
        "\n",
        "# Apply LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# Create a DataFrame with LDA results\n",
        "df_lda = pd.DataFrame(data=X_lda, columns=['LDA1', 'LDA2'])\n",
        "df_lda['Class'] = y\n",
        "\n",
        "# Save the LDA results to a CSV file\n",
        "df_lda.to_csv('Iris_LDA_Result.csv', index=False)\n",
        "\n",
        "df_lda.head()\n",
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Load the dataset\n",
        "iris_data = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = iris_data.iloc[:, 1:-1]  # Features (excluding the first column and the last column)\n",
        "y = iris_data.iloc[:, -1]    # Labels (last column)\n",
        "\n",
        "# Apply LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# Create a DataFrame with LDA results\n",
        "df_lda = pd.DataFrame(data=X_lda, columns=['LDA1', 'LDA2'])\n",
        "df_lda['Class'] = y\n",
        "\n",
        "# Save the LDA results to a CSV file\n",
        "df_lda.to_csv('Iris_LDA_Result.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the LDA result\n",
        "print(df_lda.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5VZ4-rqDs60",
        "outputId": "77b4f263-1642-44d9-a077-c4332d3906c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       LDA1      LDA2        Class\n",
            "0  8.084953 -0.328454  Iris-setosa\n",
            "1  7.147163  0.755473  Iris-setosa\n",
            "2  7.511378  0.238078  Iris-setosa\n",
            "3  6.837676  0.642885  Iris-setosa\n",
            "4  8.157814 -0.540639  Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Briefly identify the difference between PCA and LDA\n"
      ],
      "metadata": {
        "id": "PHacjk93Eler"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both PCA and LDA are dimensionality reduction techniques, but they have different goals and methods:\n",
        "\n",
        "Principal Component Analysis (PCA):\n",
        "-PCA transforms into a new set of uncorrelated variables called principal components.These components capture the maximum variance in the data.\n",
        "-unsupervised technique.\n",
        "-PCA maximizes the variance captured in the data without considering class separation.\n",
        "\n",
        "Linear Discriminant Analysis (LDA):\n",
        "-aims to reduce the dimensionality of the data while maximizing the separation between different classes. It seeks to project the data onto a lower-dimensional space with good class-separability.\n",
        "-supervised technique,considers class labels to find the best discriminant features.\n",
        "-LDA maximizes the separation between different classes in the lower-dimensional space."
      ],
      "metadata": {
        "id": "X1CRnEi5FAgs"
      }
    }
  ]
}